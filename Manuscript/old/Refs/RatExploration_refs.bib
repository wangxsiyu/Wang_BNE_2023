@Article{wilson2014,
   Author="Wilson, R. C.  and Geana, A.  and White, J. M.  and Ludvig, E. A.  and Cohen, J. D. ",
   Title="{{H}umans use directed and random exploration to solve the explore-exploit dilemma}",
   Journal="J Exp Psychol Gen",
   Year="2014",
   Volume="143",
   Number="6",
   Pages="2074--2081",
   Month="Dec"
}
@article{Wilson2020,
author = {Wilson, Robert C and Bonawitz, Elizabeth and Costa, Vincent D},
file = {:Users/wang/Downloads/ExploreExploitOpinion.pdf:pdf},
pages = {1--18},
Year = "2020",
title = {{Balancing exploration and exploitation with information and randomization.}}
}
@article{Mehlhorn2015,
abstract = {Many decisions in the lives of animals and humans require a fine balance between the exploration of different options and the exploitation of their rewards. Do you buy the advertised car, or do you test drive different models? Do you continue feeding from the current patch of flowers, or do you fly offto another one? Do you marry your current partner, or try your luck with someone else? The balance required in these situations is commonly referred to as the exploration- exploitation tradeoff. It features prominently in a wide range of research traditions, including learning, foraging, and decision making literatures. Here, we integrate findings from these and other often-isolated literatures in order to gain a better understanding of the possible tradeoffs between exploration and exploitation, and we propose new theoretical insights that might guide future research. Specifically, we explore how potential tradeoffs depend on (a) the conceptualization of exploration and exploitation; (b) the influencing environmental, social, and individual factors; (c) the scale at which exploration and exploitation are considered; (d) the relationship and types of transitions between the 2 behaviors; and (e) the goals of the decision maker. We conclude that exploration and exploitation are best conceptualized as points on a continuum, and that the extent to which an agent's behavior can be interpreted as exploratory or exploitative depends upon the level of abstraction at which it is considered.},
author = {Mehlhorn, Katja and Newell, Ben R. and Todd, Peter M. and Lee, Michael D. and Morgan, Kate and Braithwaite, Victoria A. and Hausmann, Daniel and Fiedler, Klaus and Gonzalez, Cleotilde},
doi = {10.1037/dec0000033},
issn = {23259973},
journal = {Decision},
keywords = {Decision making,Decision theory,Exploration- exploitation tradeoff,Foraging,Learning},
title = {{Unpacking the exploration-exploitation tradeoff: A synthesis of human and animal literatures}},
year = {2015}
}

@article{Nissen1930,
author = {Nissen, Henry W.},
doi = {10.1080/08856559.1930.9944162},
file = {:Volumes/Wang/Library/Literatures/Exploration-Exploitation tradeoff/Rodent studies/Nissen - 1930 - A Study of Exploratory Behavior in the White Rat by Means of the Obstruction Method - Pedagogical Seminary and Journal o.pdf:pdf},
issn = {08856559},
journal = {Pedagogical Seminary and Journal of Genetic Psychology},
mendeley-groups = {Exploration/Exploitation tradeoff/Rodent EE},
number = {3},
pages = {361--376},
title = {{A Study of Exploratory Behavior in the White Rat by Means of the Obstruction Method}},
volume = {37},
year = {1930}
}
@article{Beeler2010,
abstract = {The impact of dopamine on adaptive behavior in a naturalistic environment is largely unexamined. Experimental work suggests that phasic dopamine is central to reinforcement learning whereas tonic dopamine may modulate performance without altering learning per se; however, this idea has not been developed formally or integrated with computational models of dopamine function. We quantitatively evaluate the role of tonic dopamine in these functions by studying the behavior of hyperdopaminergic DAT knockdown mice in an instrumental task in a seminaturalistic homecage environment. In this "closed economy" paradigm, subjects earn all of their food by pressing either of two levers, but the relative cost for food on each lever shifts frequently. Compared to wild-type mice, hyperdopaminergic mice allocate more lever presses on high-cost levers, thus working harder to earn a given amount of food and maintain their body weight. However, both groups show a similarly quick reaction to shifts in lever cost, suggesting that the hyperdominergic mice are not slower at detecting changes, as with a learning deficit. We fit the lever choice data using reinforcement learning models to assess the distinction between acquisition and expression the models formalize. In these analyses, hyperdopaminergic mice displayed normal learning from recent reward history but diminished capacity to exploit this learning: a reduced coupling between choice and reward history. These data suggest that dopamine modulates the degree to which prior learning biases action selection and consequently alters the expression of learned, motivated behavior. {\textcopyright} 2010 Beeler, Daw, Frazier and Zhuang.},
author = {Beeler, Jeff A. and Daw, Nathaniel and Frazier, Cristianne R.M. and Zhuang, Xiaoxi},
doi = {10.3389/fnbeh.2010.00170},
file = {:Volumes/Wang/Library/Literatures/Exploration-Exploitation tradeoff/Rodent studies/Beeler et al. - 2010 - Tonic dopamine modulates exploitation of reward learning - Frontiers in Behavioral Neuroscience.pdf:pdf},
issn = {16625153},
journal = {Frontiers in Behavioral Neuroscience},
keywords = {Behavioral flexibility,DAT knock-down,Dopamine,Environmental adaptation,Explore-exploit,Reinforcement learning},
mendeley-groups = {Exploration/Exploitation tradeoff/Rodent EE},
number = {NOV},
pages = {1--14},
title = {{Tonic dopamine modulates exploitation of reward learning}},
volume = {4},
year = {2010}
}
@article{Braz2015,
abstract = {Findings showing that neonatal lesions of the forebrain dopaminergic system in rodents lead to juvenile locomotor hyperactivity and learning deficits have been taken as evidence of face validity for the attention deficit hyperactivity disorder. However, the core cognitive and physiological intermediate phenotypes underlying this rodent syndrome remain unknown. Here we show that early postnatal dopaminergic lesions cause long-lasting deficits in exploitation of shelter, social and nutritional resources, and an imbalanced exploratory behavior, where nondirected local exploration is exacerbated, whereas sophisticated search behaviors involving sequences of goal directed actions are degraded. Importantly, some behavioral deficits do not diminish after adolescence but instead worsen or mutate, particularly those related to the exploration of wide and spatially complex environments. The in vivo electrophysiological recordings and morphological reconstructions of striatal medium spiny neurons reveal corticostriatal alterations associated to the behavioral phenotype. More specifically, an attenuation of corticostriatal functional connectivity, affecting medial prefrontal inputs more markedly than cingulate and motor inputs, is accompanied by a contraction of the dendritic arbor of striatal projection neurons in this animal model. Thus, dopaminergic neurons are essential during postnatal development for the functional and structural maturation of corticostriatal connections. From a bottom-up viewpoint, our findings suggest that neuropsychiatric conditions presumably linked to developmental alterations of the dopaminergic system should be evaluated for deficits in foraging decision making, alterations in the recruitment of corticostriatal circuits during foraging tasks, and structural disorganization of the frontostriatal connections.},
author = {Braz, Barbara Y. and Galina{\~{n}}es, Gregorio L. and Taravini, Irene R.E. and Belforte, Juan E. and Murer, M. Gustavo},
doi = {10.1038/npp.2015.104},
file = {:Volumes/Wang/Library/Literatures/Exploration-Exploitation tradeoff/Rodent studies/Braz et al. - 2015 - Altered Corticostriatal Connectivity and ExplorationExploitation Imbalance Emerge as Intermediate Phenotypes for a.pdf:pdf},
issn = {1740634X},
journal = {Neuropsychopharmacology},
mendeley-groups = {Exploration/Exploitation tradeoff/Rodent EE},
number = {11},
pages = {2576--2587},
title = {{Altered Corticostriatal Connectivity and Exploration/Exploitation Imbalance Emerge as Intermediate Phenotypes for a Neonatal Dopamine Dysfunction}},
volume = {40},
year = {2015}
}
@article{Laskowski2016,
abstract = {The medial prefrontal cortex (mPFC) plays a major role in goal-directed behaviours, but it is unclear whether it plays a role in breaking away from a high-value reward in order to explore for better options. To address this question, we designed a novel 3-arm Bandit Task in which rats were required to choose one of three potential reward arms, each of which was associated with a different amount of food reward and time-out punishment. After a variable number of choice trials the reward locations were shuffled and animals had to disengage from the now devalued arm and explore the other options in order to optimise payout. Lesion and control groups' behaviours on the task were then analysed by fitting data with a reinforcement learning model. As expected, lesioned animals obtained less reward overall due to an inability to flexibly adapt their behaviours after a change in reward location. However, modelling results showed that lesioned animals were no more likely to explore than control animals. We also discovered that all animals showed a strong preference for certain maze arms, at the expense of reward. This tendency was exacerbated in the lesioned animals, with the strongest effects seen in a subset of animals with damage to dorsal mPFC. The results confirm a role for mPFC in goal-directed behaviours but suggest that rats rely on other areas to resolve the explore-exploit dilemma.},
author = {Laskowski, C. S. and Williams, R. J. and Martens, K. M. and Gruber, A. J. and Fisher, K. G. and Euston, D. R.},
doi = {10.1016/j.bbr.2016.03.007},
file = {:Volumes/Wang/Library/Literatures/Exploration-Exploitation tradeoff/Rodent studies/Laskowski et al. - 2016 - The role of the medial prefrontal cortex in updating reward value and avoiding perseveration - Behavioural Bra.pdf:pdf},
issn = {18727549},
journal = {Behavioural Brain Research},
keywords = {Decision making,Exploration,Prefrontal cortex,Reinforcement learning,Reward,Value},
mendeley-groups = {Exploration/Exploitation tradeoff/Rodent EE},
pages = {52--63},
publisher = {Elsevier B.V.},
title = {{The role of the medial prefrontal cortex in updating reward value and avoiding perseveration}},
url = {http://dx.doi.org/10.1016/j.bbr.2016.03.007},
volume = {306},
year = {2016}
}
@article{Parker2016,
abstract = {Dopaminergic (DA) neurons in the midbrain provide rich topographic innervation of the striatum and are central to learning and to generating actions. Despite the importance of this DA innervation, it remains unclear whether and how DA neurons are specialized on the basis of the location of their striatal target. Thus, we sought to compare the function of subpopulations of DA neurons that target distinct striatal subregions in the context of an instrumental reversal learning task. We identified key differences in the encoding of reward and choice in dopamine terminals in dorsal versus ventral striatum: DA terminals in ventral striatum responded more strongly to reward consumption and reward-predicting cues, whereas DA terminals in dorsomedial striatum responded more strongly to contralateral choices. In both cases the terminals encoded a reward prediction error. Our results suggest that the DA modulation of the striatum is spatially organized to support the specialized function of the targeted subregion.},
author = {Parker, Nathan F. and Cameron, Courtney M. and Taliaferro, Joshua P. and Lee, Junuk and Choi, Jung Yoon and Davidson, Thomas J. and Daw, Nathaniel D. and Witten, Ilana B.},
doi = {10.1038/nn.4287},
file = {:Volumes/Wang/Library/Literatures/Exploration-Exploitation tradeoff/Rodent studies/Parker et al. - 2016 - Reward and choice encoding in terminals of midbrain dopamine neurons depends on striatal target - Nature Neurosci.pdf:pdf},
issn = {15461726},
journal = {Nature Neuroscience},
mendeley-groups = {Exploration/Exploitation tradeoff/Rodent EE},
number = {6},
pages = {845--854},
title = {{Reward and choice encoding in terminals of midbrain dopamine neurons depends on striatal target}},
volume = {19},
year = {2016}
}
@article{Monk2018,
abstract = {Understanding how humans and other animals behave in response to changes in their environments is vital for predicting population dynamics and the trajectory of coupled social-ecological systems. Here, we present a novel framework for identifying emergent social behaviours in foragers (including humans engaged in fishing or hunting) in predator–prey contexts based on the exploration difficulty and exploitation potential of a renewable natural resource. A qualitative framework is introduced that predicts when foragers should behave territorially, search collectively, act independently or switch among these states. To validate it, we derived quantitative predictions from two models of different structure: a generic mathematical model, and a lattice-based evolutionary model emphasising exploitation and exclusion costs. These models independently identified that the exploration difficulty and exploitation potential of the natural resource controls the social behaviour of resource exploiters. Our theoretical predictions were finally compared to a diverse set of empirical cases focusing on fisheries and aquatic organisms across a range of taxa, substantiating the framework's predictions. Understanding social behaviour for given social-ecological characteristics has important implications, particularly for the design of governance structures and regulations to move exploited systems, such as fisheries, towards sustainability. Our framework provides concrete steps in this direction.},
author = {Monk, Christopher T. and Barbier, Matthieu and Romanczuk, Pawel and Watson, James R. and Al{\'{o}}s, Josep and Nakayama, Shinnosuke and Rubenstein, Daniel I. and Levin, Simon A. and Arlinghaus, Robert},
doi = {10.1111/ele.12949},
file = {:Volumes/Wang/Library/Literatures/Exploration-Exploitation tradeoff/Rodent studies/Monk et al. - 2018 - How ecology shapes exploitation a framework to predict the behavioural response of human and animal foragers alon.pdf:pdf},
issn = {14610248},
journal = {Ecology Letters},
keywords = {Conflict,consumer-resource,cooperation,fish and fisheries,governance,human behaviour,predator–prey,social-ecological system,sustainability},
mendeley-groups = {Exploration/Exploitation tradeoff/Rodent EE},
number = {6},
pages = {779--793},
pmid = {29611278},
title = {{How ecology shapes exploitation: a framework to predict the behavioural response of human and animal foragers along exploration–exploitation trade-offs}},
volume = {21},
year = {2018}
}
@article{Koralek2019,
abstract = {We are constantly faced with the trade-off between exploiting past actions with known outcomes and exploring novel actions whose outcomes may be better. The balance between exploitation and exploration has been hypothesized to rely on multiple neuromodulator systems, namely dopaminergic neurons of the substantia nigra pars compacta (SNc) and noradrenergic neurons of the locus coeruleus (LC). However, little is known about the dynamics of these neuromodulator systems during exploitative and exploratory states, or how they interact. We developed a novel behavioral paradigm to capture exploitative and exploratory behavioral states, and imaged calcium dynamics in genetically-identified dopaminergic SNc neurons and noradrenergic LC neurons during the transitions between these states. We found dichotomous changes in sustained activity in SNc and LC during exploitative bouts of action-reward, with SNc showing higher and LC showing lower sustained activity. Exploitative states were also marked by a lengthening of positive SNc response plateaus and negative LC response depressions, as well as hysteretic dynamics in SNc networks. Chemogenetic enhancement of dopaminergic and noradrenergic excitability favored exploitative and exploratory states, respectively. Together, these data suggest that opponent changes in dopaminergic and noradrenergic activity states modulate the transitions between exploitative and exploratory behavioral states, with important implications for downstream circuit dynamics.},
author = {Koralek, Aaron C. and Costa, Rui M.},
doi = {10.1101/822650},
file = {:Volumes/Wang/Library/Literatures/Exploration-Exploitation tradeoff/Rodent studies//Koralek, Costa - 2019 - Sustained Dopaminergic Plateaus and Noradrenergic Depressions Bias Transitions into Exploitative Behavioral Stat.pdf:pdf},
journal = {bioRxiv},
mendeley-groups = {Exploration/Exploitation tradeoff/Rodent EE},
title = {{Sustained Dopaminergic Plateaus and Noradrenergic Depressions Bias Transitions into Exploitative Behavioral States}},
year = {2019}
}
@article{Cinotti2019,
abstract = {In a volatile environment where rewards are uncertain, successful performance requires a delicate balance between exploitation of the best option and exploration of alternative choices. It has theoretically been proposed that dopamine contributes to the control of this exploration-exploitation trade-off, specifically that the higher the level of tonic dopamine, the more exploitation is favored. We demonstrate here that there is a formal relationship between the rescaling of dopamine positive reward prediction errors and the exploration-exploitation trade-off in simple non-stationary multi-armed bandit tasks. We further show in rats performing such a task that systemically antagonizing dopamine receptors greatly increases the number of random choices without affecting learning capacities. Simulations and comparison of a set of different computational models (an extended Q-learning model, a directed exploration model, and a meta-learning model) fitted on each individual confirm that, independently of the model, decreasing dopaminergic activity does not affect learning rate but is equivalent to an increase in random exploration rate. This study shows that dopamine could adapt the exploration-exploitation trade-off in decision-making when facing changing environmental contingencies.},
author = {Cinotti, Fran{\c{c}}ois and Fresno, Virginie and Aklil, Nassim and Coutureau, Etienne and Girard, Beno{\^{i}}t and Marchand, Alain R. and Khamassi, Mehdi},
doi = {10.1038/s41598-019-43245-z},
file = {:Volumes/Wang/Library/Literatures/Exploration-Exploitation tradeoff/Rodent studies/Cinotti et al. - 2019 - Dopamine blockade impairs the exploration-exploitation trade-off in rats - j.pdf:pdf},
isbn = {4159801943},
issn = {20452322},
journal = {Scientific Reports},
mendeley-groups = {Exploration/Exploitation tradeoff/Rodent EE},
number = {1},
pages = {1--14},
pmid = {31043685},
title = {{Dopamine blockade impairs the exploration-exploitation trade-off in rats}},
volume = {9},
year = {2019}
}
@article{Jackson2020,
abstract = {During self-guided behaviors, animals identify constraints of the problems they face and adaptively employ appropriate strategies (Marsh, 2002). In the case of foraging, animals must balance sensory-guided exploration of an environment with memory-guided exploitation of known resource locations. Here, we show that animals adaptively shift cognitive resources between sensory and memory systems during foraging to optimize route planning under uncertainty. We demonstrate this using a new, laboratory-based discovery method to define the strategies used to solve a difficult route optimization scenario, the probabilistic "traveling salesman" problem (Raman and Gill, 2017; Fuentes et al., 2018; Mukherjee et al., 2019). Using this system, we precisely manipulated the strength of prior information as well as the complexity of the problem. We find that rats are capable of efficiently solving this route-planning problem, even under conditions with unreliable prior information and a large space of possible solutions. Through analysis of animals' trajectories, we show that they shift the balance between exploiting known locations and searching for new locations of rewards based on the predictability of reward locations. When compared with a Bayesian search, we found that animal performance is consistent with an approach that adaptively allocates cognitive resources between sensory processing and memory, enhancing sensory acuity and reducing memory load under conditions in which prior information is unreliable. Our findings establish new approaches to understand neural substrates of natural behavior as well as the rational development of biologically inspired approaches for complex real-world optimization.},
author = {Jackson, Brian J. and Fatima, Gusti Lulu and Oh, Sujean and Gire, David H.},
doi = {10.1523/ENEURO.0536-19.2020},
file = {:Volumes/Wang/Library/Literatures/Exploration-Exploitation tradeoff/Rodent studies/Jackson et al. - 2020 - Many Paths to the Same Goal Balancing Exploration and Exploitation during Probabilistic Route Planning - eNeuro.pdf:pdf},
issn = {23732822},
journal = {eNeuro},
keywords = {Bayesian,foraging,navigation},
mendeley-groups = {Exploration/Exploitation tradeoff/Rodent EE},
number = {3},
pages = {1--11},
pmid = {32414790},
title = {{Many Paths to the Same Goal: Balancing Exploration and Exploitation during Probabilistic Route Planning}},
volume = {7},
year = {2020}
}
@article{Verharen2020,
abstract = {Rationale: During value-based decision-making, organisms make choices on the basis of reward expectations, which have been formed during prior action-outcome learning. Although it is known that neuronal manipulations of different subregions of the rat prefrontal cortex (PFC) have qualitatively different effects on behavioral tasks involving value-based decision-making, it is unclear how these regions contribute to the underlying component processes. Objectives: Assessing how different regions of the rodent PFC contribute to component processes of value-based decision-making behavior, including reward (or positive feedback) learning, punishment (or negative feedback) learning, response persistence, and exploration versus exploitation. Methods: We performed behavioral modeling of data of rats in a probabilistic reversal learning task after pharmacological inactivation of five PFC subregions, to assess how inactivation of these different regions affected the structure of responding of animals in the task. Results: Our results show reductions in reward and punishment learning after PFC subregion inactivation. The prelimbic, infralimbic, lateral orbital, and medial orbital PFC particularly contributed to punishment learning, and the prelimbic and lateral orbital PFC to reward learning. In addition, response persistence depended on the infralimbic and medial orbital PFC. As a result, pharmacological inactivation of the infralimbic and lateral orbitofrontal cortex reduced the number of reversals achieved, whereas inactivation of the prelimbic and medial orbitofrontal cortex decreased the number of rewards obtained. Finally, using simulated data, we explain discrepancies with a previous study and demonstrate complex, interacting relationships between conventional measures of probabilistic reversal learning performance, such as win-stay/lose-switch behavior, and component processes of value-based decision-making. Conclusions: Together, our data suggest that distinct components of value-based learning and decision-making are generated in medial and orbital PFC regions, displaying functional specialization and overlap, with a prominent role of large parts of the PFC in negative feedback processing.},
author = {Verharen, Jeroen P.H. and den Ouden, Hanneke E.M. and Adan, Roger A.H. and Vanderschuren, Louk J.M.J.},
doi = {10.1007/s00213-020-05454-7},
file = {:Volumes/Wang/Library/Literatures/Exploration-Exploitation tradeoff/Rodent studies/Verharen et al. - 2020 - Modulation of value-based decision making behavior by subregions of the rat prefrontal cortex - Psychopharmacol.pdf:pdf},
issn = {14322072},
journal = {Psychopharmacology},
keywords = {Behavioral modeling,Decision-making,Prefrontal cortex,Punishment,Rats,Reinforcement learning,Reward,Value},
mendeley-groups = {Exploration/Exploitation tradeoff/Rodent EE},
number = {5},
pages = {1267--1280},
pmid = {32025777},
publisher = {Psychopharmacology},
title = {{Modulation of value-based decision making behavior by subregions of the rat prefrontal cortex}},
volume = {237},
year = {2020}
}


@article{Kacelnik1979,
author = {Kacelnik, Alejandro.},
file = {:Volumes/Wang{\_}Lab/Library/Literature/Exploration/Kacelnik - 1979 - Studies of foraging, behaviour and time budgeting in great tits (Parus major) - Unknown.pdf:pdf},
mendeley-groups = {Exploration},
title = {{Studies of foraging, behaviour and time budgeting in great tits (Parus major)}},
year = {1979},
journal = {University of Oxford, Ph.D dissertation}
}
@article{Krebs1978,
abstract = {When great tits forage in an unknown environment containing two feeding places of different profitability, they first sample the two places and then exploit the more profitable one. The balance between sampling and exploitation shown by the birds is close to an optimal solution for maximising the number of food-items obtained during a feeding period. {\textcopyright} 1978 Nature Publishing Group.},
author = {Krebs, John R. and Kacelnik, Alejandro and Taylor, Peter},
doi = {10.1038/275027a0},
file = {:Volumes/Wang{\_}Lab/Library/Literature/Exploration/Krebs, Kacelnik, Taylor - 1978 - Test of optimal sampling by foraging great tits - Nature.pdf:pdf},
issn = {00280836},
journal = {Nature},
mendeley-groups = {Exploration},
number = {5675},
pages = {27--31},
title = {{Test of optimal sampling by foraging great tits}},
volume = {275},
year = {1978}
}
@book{SuttonBarto, author = {Sutton, Richard S. and Barto, Andrew G.}, title = {Reinforcement Learning: An Introduction}, year = {2018}, isbn = {0262039249}, publisher = {A Bradford Book}, address = {Cambridge, MA, USA} }
@article{Thompson1933,
abstract = {IN elaborating the relations of the present conmmunication interest was not centred upon the interpretation of particular data, but grew out of a general interest in problems of research planning. From this point of view there can be no objection to the use of data, however meagre, as a guide to action required before more can be collected; although serious objection can otherwise be raised to argument based upon a small number of observations. Indeed, the fact that such objection can never be eliminated entirely-no matter how great the number of observations-suggested the possible value of seeking other modes of operation than that of taking a large number of observations before analysis or any attemipt to direct our course. This problem is more general than that treated in Section 2, and is directly con-cerned with any case where probability criteria may be established by means of which we judge whether one mode of operation is better than another in some given sense or not. Thus, if, in this sense, P is the probability estimnate that one treatment of a certain class of individuals is better than a second, as judged by data at present available, then we might take some monotone increasing function of P, sayf(p), to fix the fraction of such individuals to be treated in the first manner; until more evidence may be utilised, where 0 {\textless}{\_} fp) {\textless} 1; the remaining fraction of such individuals (1 -f(p)) to be treated in the second manner; or we may establish a probability of treatment by the two methods of f(p) and 1 -f(p), respectively. If such a discipline were adopted, even though it were not the best possible, it seems apparent that a considerable saving of individuals otherwise sacrificed to the inferior treatment might be effected. This would be imnportant in cases where either the rate of accumulation of data is slow or the individuals treated are valuable, or both. If we arbitrarily decide to eliminate the second treatment in favour of the first at this time, then the expectation of sacrifice to the inferior treatment would be (1 -P) for all subsequently treated individuals; whereas, if, for example, we take f(p) = P, the expectation of such sacrifice would be temporartly This content downloaded from 143.89.58.9 on Thu, 30 Nov 2017 08:26:26 UTC All use subject to http://about.jstor.org/terms},
author = {Thompson, William R.},
doi = {10.2307/2332286},
issn = {00063444},
journal = {Biometrika},
mendeley-groups = {Exploration},
title = {{On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples}},
year = {1933}
}
@article{Auer2002,
abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
author = {Auer, Peter and Cesa-Bianchi, Nicol{\`{o}} and Fischer, Paul},
doi = {10.1023/A:1013689704352},
file = {:Volumes/Wang{\_}Lab/Library/Literature/Exploration/Auer2002{\_}Article{\_}Finite-timeAnalysisOfTheMultia.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Adaptive allocation rules,Bandit problems,Finite horizon regret},
mendeley-groups = {Exploration},
title = {{Finite-time analysis of the multiarmed bandit problem}},
year = {2002}
}
@article{Tomov2020,
abstract = {Most real-world decisions involve a delicate balance between exploring unfamiliar alternatives and committing to the best known option. Previous work has shown that humans rely on different forms of uncertainty to negotiate this "explore-exploit” trade-off, yet the neural basis of the underlying computations remains unclear. Using fMRI (n = 31), we find that relative uncertainty is represented in right rostrolateral prefrontal cortex and drives directed exploration, while total uncertainty is represented in right dorsolateral prefrontal cortex and drives random exploration. The decision value signal combining relative and total uncertainty to compute choice is reflected in motor cortex activity. The variance of this signal scales with total uncertainty, consistent with a sampling mechanism for random exploration. Overall, these results are consistent with a hybrid computational architecture in which different uncertainty computations are performed separately and then combined by downstream decision circuits to compute choice.},
author = {Tomov, Momchil S. and Truong, Van Q. and Hundia, Rohan A. and Gershman, Samuel J.},
doi = {10.1038/s41467-020-15766-z},
issn = {20411723},
journal = {Nature Communications},
mendeley-groups = {Exploration},
pmid = {32398675},
title = {{Dissociable neural correlates of uncertainty underlie different exploration strategies}},
year = {2020}
}
@article{Gershman2018,
abstract = {The dilemma between information gathering (exploration) and reward seeking (exploitation) is a fundamental problem for reinforcement learning agents. How humans resolve this dilemma is still an open question, because experiments have provided equivocal evidence about the underlying algorithms used by humans. We show that two families of algorithms can be distinguished in terms of how uncertainty affects exploration. Algorithms based on uncertainty bonuses predict a change in response bias as a function of uncertainty, whereas algorithms based on sampling predict a change in response slope. Two experiments provide evidence for both bias and slope changes, and computational modeling confirms that a hybrid model is the best quantitative account of the data.},
author = {Gershman, Samuel J.},
doi = {10.1016/j.cognition.2017.12.014},
file = {:Volumes/Wang{\_}Lab/Library/Literature/Exploration/Deconstructing the human algorithms for exploration | Elsevier Enhanced Reader.pdf:pdf;:Volumes/Wang{\_}Lab/Library/Literature/Exploration/Gershman - 2018 - Deconstructing the human algorithms for exploration - Cognition.pdf:pdf},
issn = {00100277},
journal = {Cognition},
keywords = {Bayesian inference,Explore-exploit dilemma,Reinforcement learning},
mendeley-groups = {Exploration},
month = {apr},
pages = {34--42},
title = {{Deconstructing the human algorithms for exploration}},
volume = {173},
year = {2018}
}
@article{Gershman2019,
abstract = {In order to discover the most rewarding actions, agents must collect information about their environment, potentially foregoing reward. The optimal solution to this "explore- exploit" dilemma is often computationally challenging, but principled algorithmic approximations exist. These approximations utilize uncertainty about action values in different ways. Some random exploration algorithms scale the level of choice stochasticity with the level of uncertainty. Other directed exploration algorithms add a "bonus" to action values with high uncertainty. Random exploration algorithms are sensitive to total uncertainty across actions, whereas directed exploration algorithms are sensitive to relative uncertainty. This article reports a multiarmed bandit experiment in which total and relative uncertainty were orthogonally manipulated. We found that humans employ both exploration strategies, and that these strategies are independently controlled by different uncertainty computations.},
author = {Gershman, Samuel J.},
doi = {10.1037/dec0000101},
issn = {23259973},
journal = {Decision},
keywords = {Bayesian inference,Explore- exploit dilemma,Reinforcement learning},
mendeley-groups = {Unread,Exploration},
title = {{Uncertainty and exploration}},
year = {2019}
}
@article{Zajkowski2017,
abstract = {The explore-exploit dilemma occurs anytime we must choose between exploring unknown options for information and exploiting known resources for reward. Previous work suggests that people use two different strategies to solve the explore-exploit dilemma: directed exploration, driven by information seeking, and random exploration, driven by decision noise. Here, we show that these two strategies rely on different neural systems. Using transcranial magnetic stimulation to inhibit the right frontopolar cortex, we were able to selectively inhibit directed exploration while leaving random exploration intact. This suggests a causal role for right frontopolar cortex in directed, but not random, exploration and that directed and random exploration rely on (at least partially) dissociable neural systems.},
author = {Zajkowski, Wojciech K. and Kossut, Malgorzata and Wilson, Robert C.},
doi = {10.7554/eLife.27430},
file = {:Volumes/Wang{\_}Lab/Library/Literature/Exploration/elife-27430-v2.pdf:pdf},
issn = {2050084X},
journal = {eLife},
mendeley-groups = {Exploration},
pages = {1--18},
pmid = {28914605},
title = {{A causal role for right frontopolar cortex in directed, but not random, exploration}},
volume = {6},
year = {2017}
}
@misc{Song2019,
abstract = {People often choose between sticking with an available good option (exploitation) and trying out a new option that is uncertain but potentially more rewarding (exploration)1,2. Laboratory studies on explore–exploit decisions often contain real-world complexities such as non-stationary environments, stochasticity under exploitation and unknown reward distributions3–7. However, such factors might limit the researcher's ability to understand the essence of people's explore–exploit decisions. For this reason, we introduce a minimalistic task in which the optimal policy is to start off exploring and to switch to exploitation at most once in each sequence of decisions. The behaviour of 49 laboratory and 143 online participants deviated both qualitatively and quantitatively from the optimal policy, even when allowing for bias and decision noise. Instead, people seem to follow a suboptimal rule in which they switch from exploration to exploitation when the highest reward so far exceeds a certain threshold. Moreover, we show that this threshold decreases approximately linearly with the proportion of the sequence that remains, suggesting a temporal ratio law. Finally, we find evidence for ‘sequence-level' variability that is shared across all decisions in the same sequence. Our results emphasize the importance of examining sequence-level strategies and their variability when studying sequential decision-making.},
author = {Song, Mingyu and Bnaya, Zahy and Ma, Wei Ji},
booktitle = {Nature Human Behaviour},
doi = {10.1038/s41562-018-0526-x},
issn = {23973374},
mendeley-groups = {Unread,Exploration},
title = {{Sources of suboptimality in a minimalistic explore–exploit task}},
year = {2019}
}
@article{Meyer1995,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. The process by which individuals learn from feedback when making recurrent choices among ambiguous alternatives is explored. We describe an experiment in which subjects solve a variant of the classic armed-bandit problem of dynamic decision theory, set in the context of airline choice. Subjects are asked to make repeated choices between two hypothetical airlines, one having an on-time departure probability which is known a priori, and the other has an ambiguous probability whose true value can only be discovered by making sample trips on the airline. Subjects attempt to make choices in such a way as to maximize the total number of one-time departures over a fixed planning horizon. We examine the extent to which actual choice patterns over time are consistent with those which would be made by a decision maker acting as an optimal Bernoulli sampler. The data offer support for a number of expected-and some unexpected-departures from optimality, including a tendency to underexperiment with promising options and overexperiment with unpromising options, and a tendency to increasingly switch between airlines as the average base rate of departures decreases. Implications of the work for the descriptive validity of normative dynamic decision models is explored, as well as for the generalizability of previous findings about choice under ambiguity to dynamic settings.},
author = {Meyer, Robert J. and Shi, Yong},
doi = {10.1287/mnsc.41.5.817},
issn = {0025-1909},
journal = {Management Science},
title = {{Sequential Choice Under Ambiguity: Intuitive Solutions to the Armed-Bandit Problem}},
year = {1995}
}
@article{Banks1997,
abstract = {We investigate, in an experimental setting, the behavior of single decision makers who at discrete time intervals over an "infinite" horizon may choose one action from a set of possible actions where this set is constant over time, i.e. a bandit problem. Two bandit environments are examined, one in which the predicted behavior should always be myopic (the two-armed bandit) and the other in which the predicted behavior should never be myopic (the one-armed bandit). We also investigate the comparative static predictions as the underlying parameters of the bandit environments are changed. The aggregate results show that the behavior in the two bandit environments are quantitatively different and in the direction of the theoretical predictions.},
author = {Banks, Jeffrey and Olson, Mark and Porter, David},
doi = {10.1007/s001990050146},
issn = {09382259},
journal = {Economic Theory},
title = {{An experimental analysis of the bandit problem}},
year = {1997}
}
@article{Frank2009,
abstract = {The basal ganglia support learning to exploit decisions that have yielded positive outcomes in the past. In contrast, limited evidence implicates the prefrontal cortex in the process of making strategic exploratory decisions when the magnitude of potential outcomes is unknown. Here we examine neurogenetic contributions to individual differences in these distinct aspects of motivated human behavior, using a temporal decision-making task and computational analysis. We show that two genes controlling striatal dopamine function, DARPP-32 (also called PPP1R1B) and DRD2, are associated with exploitative learning to adjust response times incrementally as a function of positive and negative decision outcomes. In contrast, a gene primarily controlling prefrontal dopamine function (COMT) is associated with a particular type of 'directed exploration', in which exploratory decisions are made in proportion to Bayesian uncertainty about whether other choices might produce outcomes that are better than the status quo. Quantitative model fits reveal that genetic factors modulate independent parameters of a reinforcement learning system. {\textcopyright} 2009 Nature America, Inc.},
author = {Frank, Michael J. and Doll, Bradley B. and Oas-Terpstra, Jen and Moreno, Francisco},
doi = {10.1038/nn.2342},
issn = {10976256},
journal = {Nature Neuroscience},
title = {{Prefrontal and striatal dopaminergic genes predict individual differences in exploration and exploitation}},
year = {2009}
}
@article{Steyvers2009,
abstract = {The bandit problem is a dynamic decision-making task that is simply described, well-suited to controlled laboratory study, and representative of a broad class of real-world problems. In bandit problems, people must choose between a set of alternatives, each with different unknown reward rates, to maximize the total reward they receive over a fixed number of trials. A key feature of the task is that it challenges people to balance the exploration of unfamiliar choices with the exploitation of familiar ones. We use a Bayesian model of optimal decision-making on the task, in which how people balance exploration with exploitation depends on their assumptions about the distribution of reward rates. We also use Bayesian model selection measures that assess how well people adhere to an optimal decision process, compared to simpler heuristic decision strategies. Using these models, we make inferences about the decision-making of 451 participants who completed a set of bandit problems, and relate various measures of their performance to other psychological variables, including psychometric assessments of cognitive abilities and personality traits. We find clear evidence of individual differences in the way the participants made decisions on the bandit problems, and some interesting correlations with measures of general intelligence. {\textcopyright} 2008 Elsevier Inc. All rights reserved.},
author = {Steyvers, Mark and Lee, Michael D. and Wagenmakers, Eric Jan},
doi = {10.1016/j.jmp.2008.11.002},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
keywords = {Bandit problem,Bayesian modeling,Decision-making,Exploration versus exploitation,Individual differences},
title = {{A Bayesian analysis of human decision-making on bandit problems}},
year = {2009}
}
@article{Lee2011,
abstract = {In bandit problems, a decision-maker must choose between a set of alternatives, each of which has a fixed but unknown rate of reward, to maximize their total number of rewards over a sequence of trials. Performing well in these problems requires balancing the need to search for highly-rewarding alternatives, with the need to capitalize on those alternatives already known to be reasonably good. Consistent with this motivation, we develop a new psychological model that relies on switching between latent exploration and exploitation states. We test the model over a range of two-alternative bandit problems, against both human and optimal decision-making data, comparing it to benchmark models from the reinforcement learning literature. By making inferences about the latent states from optimal decision-making behavior, we characterize how people should switch between exploration and exploitation. By making inferences from human data, we begin to characterize how people actually do switch. We discuss the implications of these findings for understanding and measuring the competing demands of exploration and exploitation in sequential decision-making. {\textcopyright} 2010 Elsevier B.V.},
author = {Lee, Michael D. and Zhang, Shunan and Munro, Miles and Steyvers, Mark},
doi = {10.1016/j.cogsys.2010.07.007},
issn = {13890417},
journal = {Cognitive Systems Research},
keywords = {Bandit problem,Exploration versus exploitation,Heuristic models,Latent state models,Reinforcement learning},
title = {{Psychological models of human and optimal performance in bandit problems}},
year = {2011}
}
@article{Payzan-Lenestour2011,
abstract = {Recently, evidence has emerged that humans approach learning using Bayesian updating rather than (model-free) reinforcement algorithms in a six-arm restless bandit problem. Here, we investigate what this implies for human appreciation of uncertainty. In our task, a Bayesian learner distinguishes three equally salient levels of uncertainty. First, the Bayesian perceives irreducible uncertainty or risk: even knowing the payoff probabilities of a given arm, the outcome remains uncertain. Second, there is (parameter) estimation uncertainty or ambiguity: payoff probabilities are unknown and need to be estimated. Third, the outcome probabilities of the arms change: the sudden jumps are referred to as unexpected uncertainty. We document how the three levels of uncertainty evolved during the course of our experiment and how it affected the learning rate. We then zoom in on estimation uncertainty, which has been suggested to be a driving force in exploration, in spite of evidence of widespread aversion to ambiguity. Our data corroborate the latter. We discuss neural evidence that foreshadowed the ability of humans to distinguish between the three levels of uncertainty. Finally, we investigate the boundaries of human capacity to implement Bayesian learning. We repeat the experiment with different instructions, reflecting varying levels of structural uncertainty. Under this fourth notion of uncertainty, choices were no better explained by Bayesian updating than by (model-free) reinforcement learning. Exit questionnaires revealed that participants remained unaware of the presence of unexpected uncertainty and failed to acquire the right model with which to implement Bayesian updating. {\textcopyright} 2011 Payzan-LeNestour, Bossaerts.},
author = {Payzan-Lenestour, Elise and Bossaerts, Peter},
doi = {10.1371/journal.pcbi.1001048},
issn = {1553734X},
journal = {PLoS Computational Biology},
pmid = {21283774},
title = {{Risk, unexpected uncertainty, and estimation uncertainty: Bayesian learning in unstable settings}},
year = {2011}
}
@inproceedings{Zhang2013,
abstract = {How humans achieve long-term goals in an uncertain environment, via repeated trials and noisy observations, is an important problem in cognitive science. We investigate this behavior in the context of a multi-armed bandit task. We compare human behavior to a variety of models that vary in their representational and computational complexity. Our result shows that subjects' choices, on a trial-totrial basis, are best captured by a "forgetful" Bayesian iterative learning model [21] in combination with a partially myopic decision policy known as Knowledge Gradient [7]. This model accounts for subjects' trial-by-trial choice better than a number of other previously proposed models, including optimal Bayesian learning and risk minimization, e-greedy and win-stay-lose-shift. It has the added benefit of being closest in performance to the optimal Bayesian model than all the other heuristic models that have the same computational complexity (all are significantly less complex than the optimal model). These results constitute an advancement in the theoretical understanding of how humans negotiate the tension between exploration and exploitation in a noisy, imperfectly known environment.},
author = {Zhang, Shunan and Yu, Angela J.},
booktitle = {Advances in Neural Information Processing Systems},
issn = {10495258},
title = {{Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting}},
year = {2013}
}

@article{Daw2006,
abstract = {Decision making in an uncertain environment poses a conflict between the opposing demands of gathering and exploiting information. In a classic illustration of this 'exploration-exploitation' dilemma, a gambler choosing between multiple slot machines balances the desire to select what seems, on the basis of accumulated experience, the richest option, against the desire to choose a less familiar option that might turn out more advantageous (and thereby provide information for improving future decisions). Far from representing idle curiosity, such exploration is often critical for organisms to discover how best to harvest resources such as food and water. In appetitive choice, substantial experimental evidence, underpinned by computational reinforcement learning (RL) theory, indicates that a dopaminergic, striatal and medial prefrontal network mediates learning to exploit. In contrast, although exploration has been well studied from both theoretical and ethological perspectives, its neural substrates are much less clear. Here we show, in a gambling task, that human subjects' choices can be characterized by a computationally well-regarded strategy for addressing the explore/exploit dilemma. Furthermore, using this characterization to classify decisions as exploratory or exploitative, we employ functional magnetic resonance imaging to show that the frontopolar cortex and intraparietal sulcus are preferentially active during exploratory decisions. In contrast, regions of striatum and ventromedial prefrontal cortex exhibit activity characteristic of an involvement in value-based exploitative decision making. The results suggest a model of action selection under uncertainty that involves switching between exploratory and exploitative behavioural modes, and provide a computationally precise characterization of the contribution of key decision-related brain systems to each of these functions. {\textcopyright} 2006 Nature Publishing Group.},
author = {Daw, Nathaniel D. and O'Doherty, John P. and Dayan, Peter and Seymour, Ben and Dolan, Raymond J.},
doi = {10.1038/nature04766},
issn = {14764687},
journal = {Nature},
mendeley-groups = {Exploration},
pmid = {16778890},
title = {{Cortical substrates for exploratory decisions in humans}},
year = {2006}
}
@article{Payzan-LeNestour2012,
abstract = {Little is known about how humans solve the exploitation/exploration trade-off. In particular, the evidence for uncertainty-driven exploration is mixed.The current study proposes a novel hypothesis of exploration that helps reconcile prior findings that may seem contradictory at first. According to this hypothesis, uncertainty-driven exploration involves a dilemma between two motives: (i) to speed up learning about the unknown, which may beget novel reward opportunities; (ii) to avoid the unknown because it is potentially dangerous. We provide evidence for our hypothesis using both behavioral and simulated data, and briefly point to recent evidence that the brain differentiates between these two motives. {\textcopyright} 2012 Payzan-LeNestour and Bossaerts.},
author = {Payzan-LeNestour, {\'{E}}lise and Bossaerts, Peter},
doi = {10.3389/fnins.2012.00150},
issn = {16624548},
journal = {Frontiers in Neuroscience},
keywords = {Bayesian learning,Estimation uncertainty,Exploration bonuses,Restless bandit problem,Unexpected uncertainty},
title = {{Do not bet on the unknown versus try to find out more: Estimation uncertainty and "unexpected uncertainty" both modulate exploration}},
year = {2012}
}
@article{Wilson2014,
abstract = {All adaptive organisms face the fundamental tradeoff between pursuing a known reward (exploitation) and sampling lesser-known options in search of something better (exploration). Theory suggests at least two strategies for solving this dilemma: a directed strategy in which choices are explicitly biased toward information seeking, and a random strategy in which decision noise leads to exploration by chance. In this work we investigated the extent to which humans use these two strategies. In our "Horizon task," participants made explore- exploit decisions in two contexts that differed in the number of choices that they would make in the future (the time horizon). Participants were allowed to make either a single choice in each game (horizon 1), or 6 sequential choices (horizon 6), giving them more opportunity to explore. By modeling the behavior in these two conditions, we were able to measure exploration-related changes in decision making and quantify the contributions of the two strategies to behavior. We found that participants were more information seeking and had higher decision noise with the longer horizon, suggesting that humans use both strategies to solve the exploration- exploitation dilemma. We thus conclude that both information seeking and choice variability can be controlled and put to use in the service of exploration.},
author = {Wilson, Robert C. and Geana, Andra and White, John M. and Ludvig, Elliot A. and Cohen, Jonathan D.},
doi = {10.1037/a0038199},
issn = {00963445},
journal = {Journal of Experimental Psychology: General},
keywords = {Decision making,Decision noise,Explore-exploit,Information bonus,Reinforcement learning},
title = {{Humans use directed and random exploration to solve the explore-exploit dilemma}},
year = {2014}
}

@Article{songbird1,
	Author="Brainard, M. S.  and Doupe, A. J. ",
	Title="{{W}hat songbirds teach us about learning}",
	Journal="Nature",
	Year="2002",
	Volume="417",
	Number="6886",
	Pages="351--358",
	Month="May"
}
@Article{songbird2,
	Author="Kao, M. H.  and Doupe, A. J.  and Brainard, M. S. ",
	Title="{{C}ontributions of an avian basal ganglia-forebrain circuit to real-time modulation of song}",
	Journal="Nature",
	Year="2005",
	Volume="433",
	Number="7026",
	Pages="638--643",
	Month="Feb"
}
@article{Wilson2020,
author = {Wilson, R. C., Wang, S., Sadeghiyeh, H., & Cohen, J. D. },
year = {2020}, 
title = {{Deep exploration as a unifying account of explore-exploit behavior.}},
doi = { https://doi.org/10.31234/osf.io/uj85c}
}
@inproceedings{Osband2016,
abstract = {Efficient exploration remains a major challenge for reinforcement learning (RL). Common dithering strategies for exploration, such as ∈-greedy, do not carry out temporally-extended (or deep) exploration; this can lead to exponentially larger data requirements. However, most algorithms for statistically efficient RL are not computationally tractable in complex environments. Randomized value functions offer a promising approach to efficient exploration with generalization, but existing algorithms are not compatible with nonlinearly parameterized value functions. As a first step towards addressing such contexts we develop bootstrapped DQN. We demonstrate that bootstrapped DQN can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy. In the Arcade Learning Environment bootstrapped DQN substantially improves learning speed and cumulative performance across most games.},
archivePrefix = {arXiv},
arxivId = {1602.04621},
author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and {Van Roy}, Benjamin},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1602.04621},
issn = {10495258},
mendeley-groups = {Cardstopping},
title = {{Deep exploration via bootstrapped DQN}},
year = {2016}
}

@article{hashem18,
  title={Lessons from a “failed” replication: The importance of taking action in exploration},
  author={Sadeghiyeh, Hashem and Wang, Siyu and Wilson, Robert C},
  journal={PsyArXiv. doi},
  volume={10},
  year={2018}
}
@article{Laureiro-Martinez2014,
abstract = {An optimal balance between efficient exploitation of available resources and creative exploration of alternatives is critical for adaptation and survival. Previous studies associated these behavioral drives with, respectively, the dopaminergic mesocorticolimbic system and frontopolar-intraparietal networks. We study the activation of these systems in two age and gender-matched groups of experienced decision-makers differing in prior professional background, with the aim to understand the neural bases of individual differences in decision-making efficiency (performance divided by response time). We compare brain activity of entrepreneurs (who currently manage the organization they founded based on their venture idea) and managers (who are constantly involved in making strategic decisions but have no venture experience) engaged in a gambling-task assessing exploitative vs. explorative decision-making. Compared with managers, entrepreneurs showed higher decision-making efficiency, and a stronger activation in regions of frontopolar cortex (FPC) previously associated with explorative choice. Moreover, activity across a network of regions previously linked to explore/exploit tradeoffs explained individual differences in choice efficiency. These results suggest new avenues for the study of individual differences in the neural antecedents of efficient decision-making. {\textcopyright} 2014 Laureiro-Mart{\'{i}}nez, Canessa, Brusoni, Zollo, Hare, Alemanno and Cappa.},
author = {Laureiro-Mart{\'{i}}nez, Daniella and Canessa, Nicola and Brusoni, Stefano and Zollo, Maurizio and Hare, Todd and Alemanno, Federica and Cappa, Stefano F.},
doi = {10.3389/fnhum.2013.00927},
issn = {16625161},
journal = {Frontiers in Human Neuroscience},
keywords = {Decision-making,Efficiency,Exploration-exploitation,Frontopolar cortex,fMRI},
title = {{Frontopolar cortex and decision-making efficiency: Comparing brain activity of experts with different professional background during an exploration-exploitation task}},
year = {2014}
}
@article{Bourdaud2008,
abstract = {This study aims to characterize the electroencephalography (EEG) correlates of exploratory behavior. Decision making in an uncertain environment raises a conflict between two opposing needs: gathering information about the environment and exploiting this knowledge in order to optimize the decision. Exploratory behavior has already been studied using functional magnetic resonance imaging (fMRI). Based on a usual paradigm in reinforcement learning, this study has shown bilateral activation in the frontal and parietal cortex. To our knowledge, no previous study has been done on it using EEG. The study of the exploratory behavior using EEG signals raises two difficulties. First, the labels of trial as exploitation or exploration cannot be directly derived from the subject action. In order to access this information, a model of how the subject makes his decision must be built. The exploration related information can be then derived from it. Second, because of the complexity of the task, its EEG correlates are not necessarily time locked with the action. So the EEG processing methods used should be designed in order to handle signals that shift in time across trials. Using the same experimental protocol as the fMRI study, results show that the bilateral frontal and parietal areas are also the most discriminant. This strongly suggests that the EEG signal also conveys information about the exploratory behavior. {\textcopyright} 2008 IEEE.},
author = {Bourdaud, Nicolas and Chavarriaga, Ricardo and G{\'{a}}lan, Ferran and Mill{\'{a}}n, Jos{\'{e}} Del R.},
doi = {10.1109/TNSRE.2008.926712},
issn = {15344320},
journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
keywords = {Decision making,Electroencephalography (EEG),Exploratory behavior,Reinforcement learning},
pmid = {19144587},
title = {{Characterizing the EEG correlates of exploratory behavior}},
year = {2008}
}
@article{Badre2012,
abstract = {How do individuals decide to act based ona rewarding status quo versus an unexplored choice that might yield a better outcome? Recent evidence suggests that individuals may strategically explore as a function of the relative uncertainty about the expected value of options. However, the neural mechanisms supporting uncertainty-driven exploration remain underspecified. The present fMRI study scanned a reinforcement learning task in which participants stop a rotating clock hand in order to win points. Reward schedules were such that expected value could increase, decrease, or remain constant with respect to time. We fit several mathematical models to subject behavior to generate trial-by-trial estimates of exploration as a function of relative uncertainty. These estimates were used to analyze our fMRI data. Results indicate that rostrolateral prefrontal cortex tracks trial-by-trial changes in relative uncertainty, and this pattern distinguished individuals who rely on relative uncertainty for their exploratory decisions versus those who do not. {\textcopyright} 2012 Elsevier Inc.},
author = {Badre, David and Doll, Bradley B. and Long, Nicole M. and Frank, Michael J.},
doi = {10.1016/j.neuron.2011.12.025},
issn = {08966273},
journal = {Neuron},
title = {{Rostrolateral prefrontal cortex and individual differences in uncertainty-driven exploration}},
year = {2012}
}
@article{Cavanagh2012,
abstract = {In order to understand the exploitation/exploration trade-off in reinforcement learning, previous theoretical and empirical accounts have suggested that increased uncertainty may precede the decision to explore an alternative option. To date, the neural mechanisms that support the strategic application of uncertainty-driven exploration remain underspecified. In this study, electroencephalography (EEG) was used to assess trial-to-trial dynamics relevant to exploration and exploitation. Theta-band activities over middle and lateral frontal areas have previously been implicated in EEG studies of reinforcement learning and strategic control. It was hypothesized that these areas may interact during top-down strategic behavioral control involved in exploratory choices. Here, we used a dynamic reward-learning task and an associated mathematical model that predicted individual response times. This reinforcement-learning model generated value-based prediction errors and trial-by-trial estimates of exploration as a function of uncertainty. Mid-frontal theta power correlated with unsigned prediction error, although negative prediction errors had greater power overall. Trial-to-trial variations in response-locked frontal theta were linearly related to relative uncertainty and were larger in individuals who used uncertainty to guide exploration. This finding suggests that theta-band activities reflect prefrontal-directed strategic control during exploratory choices. {\textcopyright} 2012 The Author.},
author = {Cavanagh, James F. and Figueroa, Christina M. and Cohen, Michael X. and Frank, Michael J.},
doi = {10.1093/cercor/bhr332},
issn = {10473211},
journal = {Cerebral Cortex},
keywords = {EEG,exploration,prediction error,reinforcement learning,uncertainty},
title = {{Frontal theta reflects uncertainty and unexpectedness during exploration and exploitation}},
year = {2012}
}
@article{Tushman2011,
abstract = {The article presents management research on strategies which allow executives to extend the life of their corporation or organization. It is noted that the percentage of U.S. corporations which exist for as long as 40 years is approximately 0.1 percent. The concept of so-called organizational ambidexterity is considered in which a company is able to maintain its competitive advantage through both the exploitation of its existing competencies and the exploration of new lines of business. Interviews are conducted with 15 senior executives on the use of this concept in their strategic planning. It is found that execution of planning such as resource allocation was more important than planning itself.},
author = {Tushman, Michael L and O'Reilly, Charles a.},
journal = {California Management Review},
title = {{Organizational Ambidexterity in Action: How Managers Explore and Exploit}},
year = {2011}
}
@inproceedings{Agarwal2009,
abstract = {We propose novel multi-armed bandit (explore/exploit) schemes to maximize total clicks on a content module published regularly on Yahoo! Intuitively, one can "explore" each candidate item by displaying it to a small fraction of user visits to estimate the item's click-through rate (CTR), and then "exploit" high CTR items in order to maximize clicks. While bandit methods that seek to find the optimal trade-off between explore and exploit have been studied for decades, existing solutions are not satisfactory for web content publishing applications where dynamic set of items with short lifetimes, delayed feedback and non-stationary reward (CTR) distributions are typical. In this paper, we develop a Bayesian solution and extend several existing schemes to our setting. Through extensive evaluation with nine bandit schemes, we show that our Bayesian solution is uniformly better in several scenarios. We also study the empirical characteristics of our schemes and provide useful insights on the strengths and weaknesses of each. Finally, we validate our results with a "side-by-side" comparison of schemes through live experiments conducted on a random sample of real user visits to Yahoo!. {\textcopyright} 2009 IEEE.},
author = {Agarwal, Deepak and Chen, Bee Chung and Elango, Pradheep},
booktitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
doi = {10.1109/ICDM.2009.52},
isbn = {9780769538952},
issn = {15504786},
title = {{Explore/exploit schemes for web content optimization}},
year = {2009}
}
@inproceedings{Celma2016,
abstract = {Were The Rolling Stones right when they said, " You can't always get what you want; but if you try sometime you get what you need " ? Recommendation systems are the crystal ball of the Internet: predicting user intentions, making sense of big data, and delivering what people are looking for before they even know they want it. Pandora radio is best known for the Music Genome Project; the most unique and richly labeled music catalog of 1.5 million+ tracks. While this content-based approach to music recommendation is extremely effective and still used today as the foundation to the leading online radio service, Pandora has also collected more than a decade of contextual listener feedback in the form of more than 65 billion thumbs from 79M+ monthly active users who have created more than 10 billion stations. This session will look at how the interdisciplinary team at Pandora goes about making sense of these massive data sets to successfully make large scale music recommendations to our listeners.},
author = {Celma, Oscar},
doi = {10.1145/2959100.2959122},
title = {{The Exploit-Explore Dilemma in Music Recommendation}},
year = {2016}
}

@inproceedings{Ericson2011,
abstract = {Innovation is vital to companies, but also difficult to perform since there are many ways to approach the subject. Typically, a balance between all issues related to innovation is suggested in literature. The empirical study presented in this paper elaborates on two strategies for innovation projects, namely to exploit existing solutions and to explore a market to develop breakthrough solutions. This is done for the purpose to discuss management implications, and thereby also make those transparent for innovation projects. The result indicates that managerial implications for radical innovation projects are to provide internal legitimacy for the projects intentions, to provide for a clear view of balancing aspects by using concepts that fit into opposite ends on a continuum, and to preserve a rich information base about users. Copyright {\textcopyright} 2002-2012 The Design Society. All rights reserved.},
author = {Ericson, {\AA}sa and Kastensson, {\AA}sa},
booktitle = {ICED 11 - 18th International Conference on Engineering Design - Impacting Society Through Engineering Design},
isbn = {9781904670230},
keywords = {Concept development,Engineering management,Innovation,Innovation projects},
title = {{Exploit and explore: Two ways of categorizing innovation projects}},
year = {2011}
}
@article{Jackson2020,
abstract = {During self-guided behaviors animals identify constraints of the problems they face and adaptively employ appropriate strategies (Marsh, 2002). In the case of foraging, animals must balance sensory-guided exploration of an environment with memory- guided exploitation of known resource locations. Here we show that animals adaptively shift cognitive resources between sensory and memory systems during foraging to optimize route planning under uncertainty. We demonstrate this using a new, laboratory-based discovery method to define the strategies used to solve a difficult route optimization scenario, the probabilistic “traveling salesman” problem (Anaya Fuentes et al., 2018; Mukherjee et al., 2019; Raman {\&} Gill, 2017). Using this system, we precisely manipulated the strength of prior information as well as the complexity of the problem. We find that rats are capable of efficiently solving this route-planning problem, even under conditions with unreliable prior information and a large space of possible solutions. Through analysis of animals' trajectories, we show that they shift the balance between exploiting known locations and searching for new locations of rewards based upon the predictability of reward locations. When compared to a Bayesian search, we found that animal performance is consistent with an approach that adaptively allocates cognitive resources between sensory processing and memory, enhancing sensory acuity and reducing memory load under conditions in which prior information is unreliable. Our findings establish new approaches to understand neural substrates of natural behavior as well as the rational development of biologically inspired approaches for complex real-world optimization

Significance Statement Animals display remarkable problem-solving abilities across a variety of complex situations. Here, we used a large, computer-controlled foraging field with precisely controlled probabilities of food resources in either repeated or random locations to test how rats determine which strategies to use to solve an extremely complicated route planning problem. We found that rats balanced exploration for novel locations of food with exploitation of known food locations to solve this problem, with the balance between exploratory and exploitative strategies governed by the amount of information available regarding resource location. Our results show how animals balance sensory input with learned information to solve complex, real-world route planning problems.},
author = {Jackson, Brian J. and Fatima, Gusti Lulu and Oh, Sujean and Gire, David H.},
doi = {10.1523/eneuro.0536-19.2020},
journal = {eneuro},
title = {{Many paths to the same goal: balancing exploration and exploitation during probabilistic route planning}},
year = {2020}
}
@article{Agrawal1995,
abstract = {In this paper we consider the multiarmed bandit problem where the arms are chosen from a subset of the real line and the mean rewards are assumed to be a continuous function of the arms. The problem with an infinite number of arms is much more difficult than the usual one with a finite number of arms because the built-in learning task is now infinite dimensional. We devise a kernel estimator-based learning scheme for the mean reward as a function of the arms. Using this learning scheme, we construct a class of certainty equivalence control with forcing schemes and derive asymptotic upper bounds on their learning loss. To the best of our knowledge, these bounds are the strongest rates yet available. Moreover, they are stronger than the o(n) required for optimality with respect to the average-cost-per-unit-time criterion.},
author = {Agrawal, Rajeev},
doi = {10.1137/s0363012992237273},
issn = {0363-0129},
journal = {SIAM Journal on Control and Optimization},
title = {{The Continuum-Armed Bandit Problem}},
year = {1995}
}
@article{Bellman1954,
author = {Bellman, Richard},
doi = {10.1090/S0002-9904-1954-09848-8},
issn = {02730979},
journal = {Bulletin of the American Mathematical Society},
title = {{The Theory of Dynamic Programming}},
year = {1954}
}
@article{Kalman1960,
abstract = {The classical filtering and prediction problem is re-examined using the Bode-Sliannon representation of random processes and the “state-transition” method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinitememory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the coefficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix. {\textcopyright} 1960 by ASME.},
author = {Kalman, R. E.},
doi = {10.1115/1.3662552},
issn = {1528901X},
journal = {Journal of Fluids Engineering, Transactions of the ASME},
title = {{A new approach to linear filtering and prediction problems}},
year = {1960}
}


